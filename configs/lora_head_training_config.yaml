# LoRA Head Training Configuration for Galaxy Sommelier
# Conservative LoRA settings for initial experiments

# Model configuration
model:
  name: "facebook/dinov2-base"  # DINOv2 base model
  num_outputs: 26               # Standardized 26 features
  dropout_rate: 0.2
  freeze_backbone: true         # Always true for LoRA

# LoRA-specific configuration
lora:
  rank: 16                      # Conservative rank (try 8, 16, 32)
  alpha: 32                     # Standard 2x rank
  dropout: 0.1                  # LoRA dropout
  target_modules:               # Standard attention modules
    - "query"
    - "value" 
    - "key"
    - "dense"
  bias: "none"                  # Conservative: no bias adaptation

# Data configuration - Mixed SDSS+DECaLS
data:
  scratch_dir: "/pscratch/sd/s/sihany/galaxy-sommelier-data"
  sdss_dir: "/pscratch/sd/s/sihany/galaxy-sommelier-data/sdss"
  decals_dir: "/pscratch/sd/s/sihany/galaxy-sommelier-data/decals"
  catalogs_dir: "/pscratch/sd/s/sihany/galaxy-sommelier-data/catalogs"
  
# Mixed dataset configuration - Use SDSS+DECaLS
mixed_data:
  sdss_fraction: 0.5  # 50% SDSS, 50% DECaLS
  use_mixed_dataset: true
  feature_set: "standard_26"  # Use standardized 26-feature mapping

# Training configuration
training:
  batch_size: 32
  num_epochs: 5                 # Short training for LoRA head
  device: "cuda"
  
# Hardware
num_workers: 4
pin_memory: true

# Optimizer configuration
optimizer:
  type: "adamw"
  learning_rate: 0.001          # Higher LR for LoRA adapters (1e-3 as float)
  weight_decay: 0.01
  betas: [0.9, 0.999]

# Scheduler configuration
scheduler:
  type: "cosine"
  T_max: 5
  eta_min: 0.000001              # 1e-6 as float

# Preprocessing configuration
preprocessing:
  image_size: 224
  normalize_mean: [0.485, 0.456, 0.406]
  normalize_std: [0.229, 0.224, 0.225]

# Augmentation configuration
augmentation:
  horizontal_flip: true
  vertical_flip: true
  rotation_degrees: 45
  brightness_factor: 0.1
  contrast_factor: 0.1

# Early stopping configuration
early_stopping:
  patience: 5
  min_delta: 1e-4
  monitor: "val_loss"

# Paths configuration
paths:
  checkpoint_dir: "/pscratch/sd/s/sihany/galaxy-sommelier-data/models/lora"
  results_dir: "/pscratch/sd/s/sihany/galaxy-sommelier-data/results/lora"
  logs_dir: "/pscratch/sd/s/sihany/galaxy-sommelier-data/logs/lora"

# Weights & Biases configuration
wandb:
  project: "galaxy-sommelier"
  entity: null  # Add your wandb username if needed 