# Galaxy Sommelier: Vision Transformer for Galaxy Morphology Classification

## Project Overview
Building a state-of-the-art galaxy morphology classifier using DINOv2 fine-tuning that generalizes well across different astronomical surveys.

**Environment**: NERSC Perlmutter (A100 GPUs)  
**Data Storage**: `/pscratch/sd/s/sihany/galaxy-sommelier-data/`  
**Target**: Universal cross-survey morphology classifier  

---

## Current Status: Phase 3 Development ðŸš§

### 26-Feature Standardization Project âœ… COMPLETED
- **Problem**: Models trained on different feature sets (SDSS: 74, Mixed: 52, Triple: 14) making comparisons invalid
- **Solution**: Identified 26 overlapping morphological features across all surveys (SDSS, DECaLS, HST, UKIDSS)
- **Result**: All configurations now use standardized 26-feature output for fair scientific comparison

### Triple Mixed Dataset Development âœ… COMPLETED  
- **Implementation**: `TripleMixedDataset` class supporting three-way survey mixing (33% SDSS, 33% DECaLS, 34% HST)
- **Files**: `scripts/triple_mixed_dataset.py`, `configs/triple_mixed_*.yaml`

### Current Issue: Dataset Sizing ðŸš§
- **Problem**: Using arbitrary dataset sizes without scientific justification
- **Need**: Implement data-driven sizing using `min(available_catalogs) * N` methodology
- **Impact**: Required for fair comparison across all model variants

---

## Key Results Summary

### Phase 1: Foundation âœ… COMPLETED
- **Training Progression**: Baseline (r=0.116) â†’ Head-trained (r=0.759) â†’ Fine-tuned (r=0.850)
- **Best Performance**: Correlation = 0.85, RÂ² = 0.72, MAE = 0.106
- **Strong Features**: Disk detection (r=0.968), Edge-on (r=0.935), Spiral (r=0.857)

### Phase 2: OOD Generalization Study âœ… COMPLETED
- **Hypothesis Confirmed**: Mixed-survey training improves cross-survey generalization
- **Key Result**: Mixed model (SDSS+DECaLS) vs SDSS-only on UKIDSS test: **0.857 vs 0.819 correlation**
- **Tools**: Unified evaluation script `scripts/compare_models.py`

---

## Repository Structure

```
galaxy-sommelier/
â”œâ”€â”€ configs/                    # Training configurations (26-feature standardized)
â”œâ”€â”€ scripts/                    # Core implementation
â”‚   â”œâ”€â”€ compare_models.py       # Model evaluation and comparison
â”‚   â”œâ”€â”€ triple_mixed_dataset.py # Three-survey dataset implementation
â”‚   â”œâ”€â”€ sdss_dataset.py         # SDSS-only dataset
â”‚   â”œâ”€â”€ mixed_dataset.py        # SDSS+DECaLS dataset
â”‚   â””â”€â”€ standard_26_features.py # Feature standardization
â”œâ”€â”€ models/                     # Saved model checkpoints
â”œâ”€â”€ benchmark_results/          # Performance evaluation results
â”œâ”€â”€ plots/                      # Visualization outputs
â”œâ”€â”€ feature_registry.py         # Feature mapping registry
â”œâ”€â”€ four_survey_feature_mapping.csv  # Cross-survey feature mapping
â””â”€â”€ requirements.txt
```

## Data Storage Structure ($PSCRATCH)

```
/pscratch/sd/s/sihany/galaxy-sommelier-data/
â”œâ”€â”€ catalogs/                   # Galaxy Zoo catalogs
â”‚   â”œâ”€â”€ gz2_hart16.csv         # SDSS Galaxy Zoo 2
â”‚   â”œâ”€â”€ decals_gz_catalog.csv  # DECaLS Galaxy Zoo
â”‚   â””â”€â”€ hst_gz_catalog.csv     # HST Galaxy Zoo
â”œâ”€â”€ images/                     # Galaxy images by survey
â”‚   â”œâ”€â”€ sdss/                  # SDSS .fits files
â”‚   â”œâ”€â”€ decals/                # DECaLS .fits files
â”‚   â””â”€â”€ hst/                   # HST .fits files
â”œâ”€â”€ models/                     # Trained model storage
â”‚   â”œâ”€â”€ best_model.pt          # SDSS-only (74 features)
â”‚   â”œâ”€â”€ mixed/best_model.pt    # SDSS+DECaLS (52 features)
â”‚   â””â”€â”€ triple/                # Future triple-mixed models
â””â”€â”€ processed/                  # Preprocessed data cache
```

---

## Immediate Next Steps

### 1. Fix Dataset Sizing ðŸ”¥ HIGH PRIORITY
- **Task**: Update all configs with principled dataset sizing approach
- **Method**: Use `min(available_catalogs) * N` for consistent, data-driven sizes
- **Files to Update**: All config files in `configs/`

### 2. Retrain All Models on 26 Features
- **Task**: Train standardized models for fair comparison
- **Variants**: SDSS-only, Mixed (SDSS+DECaLS), Triple (SDSS+DECaLS+HST)
- **Goal**: Direct performance comparison on identical feature sets

### 3. Comprehensive Benchmarking
- **Task**: Run `compare_models.py` on all standardized models
- **Test Set**: UKIDSS (held-out survey)
- **Output**: Final performance comparison table

---

## Future Phases

### Phase 4: Cross-Survey Evaluation
- Test all models on UKIDSS with standardized 26 features
- Quantify improvement from survey diversity
- Document final scientific conclusions

### Phase 5: Production Deployment
- Model registry and versioning system
- REST API for morphology classification
- Automated monitoring and performance tracking

---

## Key Lessons Learned

1. **Feature Standardization Critical**: Models must predict identical feature sets for valid comparison
2. **Dataset Sizing Principles**: Use data-driven methodology, not arbitrary sizes
3. **Survey Diversity Helps**: Mixed training significantly improves cross-survey generalization
4. **Fair Comparison Essential**: Scientific conclusions require controlled experimental conditions