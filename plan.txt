# Galaxy Sommelier: Vision Transformer for Galaxy Morphology Classification

## Project Overview
Build a state-of-the-art galaxy morphology classifier using DINOv2 fine-tuning that generalizes well across different astronomical surveys. The project demonstrates the power of vision foundation models for astronomical applications.

**Duration**: 8-12 weeks  
**Compute Resources**: NERSC Perlmutter (A100 GPUs)  
**Datasets**: Galaxy Zoo (SDSS, DES, DECaLS, HST, UKIDSS)  
**Target**: Universal cross-survey morphology classifier  
**Data Storage**: `/pscratch/sd/s/sihany/galaxy-sommelier-data/`

---
## LATEST UPDATE: Phase 3 Development ðŸš§ (Triple Mixed Dataset & Feature Standardization)

### Triple Mixed Dataset Development
- **Objective**: Expand training to include HST data alongside SDSS+DECaLS for improved morphological diversity.
- **Implementation**: Created `TripleMixedDataset` class supporting three-way survey mixing (33% SDSS, 33% DECaLS, 34% HST).
- **Key Files**:
  - `scripts/triple_mixed_dataset.py`: New dataset class with standardized feature extraction
  - `scripts/run_triple_mixed_training.sh`: Two-stage training pipeline (head â†’ full fine-tuning)
  - `configs/triple_mixed_head_training_config.yaml` & `triple_mixed_full_finetuning_config.yaml`: Training configurations

### 26-Feature Standardization Project âœ…
- **Problem Identified**: Models trained on different feature sets (SDSS: 74, Mixed: 52, Triple: 14) making comparisons scientifically invalid.
- **Solution**: Comprehensive feature mapping across all 4 surveys (SDSS, DECaLS, HST, UKIDSS) identifying 26 overlapping morphological features.
- **Implementation**:
  - `four_survey_feature_mapping.csv`: Complete cross-survey feature mapping
  - `standard_26_features.py`: Standardized feature definitions and column mappings
  - Updated ALL model configurations to use exactly 26 features in identical order
- **Impact**: Ensures fair scientific comparison across all model variants

### Configuration Standardization âœ…
Updated all training configurations for 26-feature consistency:
- **SDSS-only models**: `head_training_config.yaml`, `full_finetuning_config.yaml`, `continued_finetuning_config.yaml`
- **Mixed SDSS+DECaLS**: `mixed_head_training_config.yaml`, `mixed_full_finetuning_config.yaml`
- **Max overlap variants**: `max_overlap_head_training_config.yaml`, `max_overlap_full_finetuning_config.yaml`, `max_overlap_improved_config.yaml`
- **High quality variants**: `mixed_high_quality_head_training_config.yaml`, `mixed_high_quality_config.yaml`
- **Triple mixed**: `triple_mixed_head_training_config.yaml`, `triple_mixed_full_finetuning_config.yaml`

### Lessons Learned: Dataset Sizing Principles
- **Issue**: Initially used arbitrary dataset size (150k galaxies) without scientific justification
- **Principle**: Dataset sizes should be data-driven and consistent with previous experiments
- **Correct Approach**: Use `min(available_catalogs) * N` methodology for principled, reproducible sizing
- **Fair Comparison**: All models should train on comparable dataset sizes for valid scientific conclusions

### Technical Improvements
- **Verified Loss Calculation**: Confirmed all models compute loss on exactly 26 features (26 predictions vs 26 targets)
- **Feature Extraction Pipeline**: Standardized feature extraction ensuring consistent output order across all surveys
- **Error Handling**: Robust catalog validation and missing feature detection
- **Memory Management**: Identified GPU memory constraints requiring principled dataset sizing

### Current Status
- âœ… All configurations updated to 26-feature standard
- âœ… Triple mixed dataset implemented with standardized features
- âœ… Feature mapping validated across all 4 surveys
- ðŸš§ Need to fix dataset sizing using principled approach
- ðŸš§ Ready to begin fair comparison training across all model variants

### Next Steps
1. **Fix Dataset Sizing**: Update configs with data-driven sizing approach
2. **Comparative Training**: Train all models on standardized 26 features
3. **Comprehensive Benchmarking**: Fair comparison across all model variants
4. **Cross-Survey Evaluation**: Test generalization on UKIDSS with consistent feature set

---
## PREVIOUS UPDATE: Phase 2 Complete âœ… (OOD Generalization Study)
- **Objective**: Tested if training on diverse data (SDSS+DECaLS) improves generalization on an unseen survey (UKIDSS).
- **Outcome**: **Success**. The mixed-survey model significantly outperformed the SDSS-only model (0.857 vs 0.819 correlation), confirming the hypothesis.
- **Additional Test**: Evaluated if high-quality SDSS data (higher classification counts) improves OOD performance. The high-quality mixed model achieved 0.855 correlation vs 0.857 for the original mixed model, showing minimal difference.
- **Key Actions**:
    - Created a unified evaluation script: `scripts/compare_models.py`.
    - Consolidated evaluation logic, replacing older scripts.
    - Updated `README.md` with final results and analysis.
---

## Project Status: Phase 1 Complete âœ…

### Achievements Summary
- **Overall Performance**: Correlation = 0.85, RÂ² = 0.72, MAE = 0.106
- **Training Progression**: Baseline (r=0.116) â†’ Head-trained (r=0.759) â†’ Fine-tuned (r=0.850)
- **Key Features Performance**:
  - Disk detection: r = 0.968 (excellent)
  - Edge-on detection: r = 0.935 (excellent)
  - Spiral detection: r = 0.857 (very good)
  - Bar detection: r = 0.772 (good)
- **Complete pipeline**: Data loading, training, benchmarking, visualization

---

## Phase 1: Foundation Setup (Week 1-2) âœ… COMPLETED

### Key Objectives
1. **Set up complete data pipeline** from Galaxy Zoo to NERSC scratch
2. **Establish baseline DINOv2 model** with custom galaxy morphology head
3. **Achieve initial training convergence** with >85% accuracy on test set
4. **Create reproducible environment** for all team members

### Implementation Steps

#### Step 1.1: Environment and Data Setup
```bash
# Create project structure on NERSC
cd $HOME
mkdir -p galaxy-sommelier/{configs,models,results,scripts,notebooks}
cd galaxy-sommelier

# Create data directories on scratch
mkdir -p /pscratch/sd/s/sihany/galaxy-sommelier-data/{sdss,desi,hst,processed}

# Create symbolic link for easy access
ln -s /pscratch/sd/s/sihany/galaxy-sommelier-data data

# Set up environment
module load python
conda create -n galaxy-sommelier python=3.10
conda activate galaxy-sommelier
```

#### Step 1.2: Galaxy Zoo Data Acquisition
```python
# scripts/download_galaxy_zoo_data.py
import os
import pandas as pd
import requests
from astropy.io import fits
from astroquery.sdss import SDSS
from tqdm import tqdm

class GalaxyZooDownloader:
    def __init__(self, scratch_dir='/pscratch/sd/s/sihany/galaxy-sommelier-data'):
        self.data_dir = os.path.join(scratch_dir, 'sdss')
        self.catalog_dir = os.path.join(scratch_dir, 'catalogs')
        os.makedirs(self.data_dir, exist_ok=True)
        os.makedirs(self.catalog_dir, exist_ok=True)
    
    def download_catalogs(self):
        """Download Galaxy Zoo catalogs"""
        print("Downloading Galaxy Zoo 2 catalogs...")
        
        # Main Galaxy Zoo 2 catalog
        gz2_url = "https://zenodo.org/record/3565489/files/gz2_hart16.csv"
        gz2_path = os.path.join(self.catalog_dir, 'gz2_hart16.csv')
        
        if not os.path.exists(gz2_path):
            response = requests.get(gz2_url)
            with open(gz2_path, 'wb') as f:
                f.write(response.content)
        
        return pd.read_csv(gz2_path)
    
    def download_sdss_images(self, catalog, limit=None):
        """Download SDSS images for Galaxy Zoo objects"""
        if limit:
            catalog = catalog.head(limit)
        
        for idx, row in tqdm(catalog.iterrows(), total=len(catalog)):
            ra, dec = row['ra'], row['dec']
            objid = row['dr7objid']
            
            output_path = os.path.join(self.data_dir, f'sdss_{objid}.fits')
            
            if os.path.exists(output_path):
                continue
                
            try:
                # Download SDSS image
                images = SDSS.get_images(coordinates=f"{ra} {dec}", 
                                       radius=30, data_release=7)
                if images:
                    images[0].writeto(output_path)
            except Exception as e:
                print(f"Error downloading {objid}: {e}")
```

#### Step 1.3: DINOv2 Model Setup
```python
# scripts/model_setup.py
import torch
import torch.nn as nn
from transformers import Dinov2Model, AutoImageProcessor

class GalaxySommelier(nn.Module):
    def __init__(self, num_outputs=37, model_name='facebook/dinov2-base', 
                 dropout_rate=0.2):
        super().__init__()
        
        # Load pre-trained DINOv2
        self.dinov2 = Dinov2Model.from_pretrained(model_name)
        self.processor = AutoImageProcessor.from_pretrained(model_name)
        
        # Get feature dimension
        hidden_size = self.dinov2.config.hidden_size
        
        # Galaxy morphology specific head with batch norm
        self.morphology_head = nn.Sequential(
            nn.Linear(hidden_size, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(256, num_outputs),
            nn.Sigmoid()  # For vote fractions [0, 1]
        )
        
        # Initialize weights
        self._init_weights()
        
    def _init_weights(self):
        """Initialize the morphology head weights"""
        for module in self.morphology_head.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                nn.init.zeros_(module.bias)
                
    def forward(self, pixel_values, return_features=False):
        outputs = self.dinov2(pixel_values=pixel_values)
        features = outputs.pooler_output
        morphology_predictions = self.morphology_head(features)
        
        if return_features:
            return morphology_predictions, features
        return morphology_predictions
```

#### Step 1.4: Data Processing Pipeline
```python
# scripts/data_processing.py
import numpy as np
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from astropy.visualization import make_lupton_rgb
import h5py

class GalaxyZooDataset(Dataset):
    def __init__(self, catalog_path, image_dir, transform=None, 
                 cache_dir='/pscratch/sd/s/sihany/galaxy-sommelier-data/processed'):
        self.catalog = pd.read_csv(catalog_path)
        self.image_dir = image_dir
        self.transform = transform
        self.cache_dir = cache_dir
        
        # Prepare morphology labels
        self.prepare_labels()
        
        # Create HDF5 cache for faster loading
        self.cache_file = os.path.join(cache_dir, 'galaxy_zoo_cache.h5')
        if not os.path.exists(self.cache_file):
            self.create_cache()
    
    def prepare_labels(self):
        """Extract and normalize morphology vote fractions"""
        # Define Galaxy Zoo 2 decision tree tasks
        self.tasks = {
            't01': ['smooth', 'features_or_disk', 'star_or_artifact'],
            't02': ['edge_on_yes', 'edge_on_no'],
            't03': ['bar_yes', 'bar_no'],
            't04': ['spiral_yes', 'spiral_no'],
            # ... add all tasks
        }
        
        # Calculate vote fractions
        for task, responses in self.tasks.items():
            cols = [f'{task}_a{i+1}_{resp}' for i, resp in enumerate(responses)]
            total = self.catalog[cols].sum(axis=1)
            for col in cols:
                self.catalog[f'{col}_frac'] = self.catalog[col] / total.clip(lower=1)
    
    def create_cache(self):
        """Create HDF5 cache for preprocessed images"""
        print(f"Creating cache at {self.cache_file}")
        
        with h5py.File(self.cache_file, 'w') as f:
            for idx in tqdm(range(len(self.catalog))):
                img = self.load_and_process_image(idx)
                if img is not None:
                    f.create_dataset(f'image_{idx}', data=img, compression='gzip')
```

#### Step 1.5: Training Infrastructure
```python
# scripts/train_baseline.py
import torch
from torch.utils.data import DataLoader
from torch.optim import AdamW
from transformers import get_linear_schedule_with_warmup
import wandb

class GalaxyTrainer:
    def __init__(self, model, config, data_dir):
        self.model = model
        self.config = config
        self.data_dir = data_dir
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Initialize wandb
        wandb.init(project="galaxy-sommelier", config=config)
        
    def setup_data_loaders(self):
        """Create train/val/test data loaders"""
        dataset = GalaxyZooDataset(
            catalog_path=os.path.join(self.data_dir, 'catalogs/gz2_hart16.csv'),
            image_dir=os.path.join(self.data_dir, 'sdss'),
            transform=self.get_transforms('train')
        )
        
        # Split dataset
        train_size = int(0.8 * len(dataset))
        val_size = int(0.1 * len(dataset))
        test_size = len(dataset) - train_size - val_size
        
        train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(
            dataset, [train_size, val_size, test_size]
        )
        
        # Create loaders
        self.train_loader = DataLoader(
            train_dataset, batch_size=self.config['batch_size'], 
            shuffle=True, num_workers=4, pin_memory=True
        )
        self.val_loader = DataLoader(
            val_dataset, batch_size=self.config['batch_size'], 
            shuffle=False, num_workers=4, pin_memory=True
        )
        
        return self.train_loader, self.val_loader
    
    def train(self, num_epochs):
        """Main training loop"""
        self.model.to(self.device)
        
        # Setup optimizer and scheduler
        optimizer = AdamW(self.model.parameters(), lr=self.config['learning_rate'])
        total_steps = len(self.train_loader) * num_epochs
        scheduler = get_linear_schedule_with_warmup(
            optimizer, num_warmup_steps=1000, num_training_steps=total_steps
        )
        
        # Training loop
        for epoch in range(num_epochs):
            # Train
            train_loss = self.train_epoch(optimizer, scheduler)
            
            # Validate
            val_loss, val_metrics = self.validate()
            
            # Log metrics
            wandb.log({
                'epoch': epoch,
                'train_loss': train_loss,
                'val_loss': val_loss,
                **val_metrics
            })
            
            # Save checkpoint
            if epoch % 5 == 0:
                self.save_checkpoint(epoch, val_loss)
```

### Deliverables for Phase 1 âœ… COMPLETED
- [x] Working data pipeline with Galaxy Zoo on NERSC scratch  
- [x] Trained baseline DINOv2 model achieving 85% morphology classification accuracy
- [x] Training infrastructure with logging and checkpointing
- [x] Performance benchmarks showing dramatic improvement: r=0.116 â†’ 0.850
- [x] Comprehensive evaluation suite with visualization tools
- [x] Model comparison analysis across training stages

---

## Phase 2: Out-of-Distribution Generalization Study (Week 3-4) âœ… COMPLETED

### Key Objectives
1.  **Hypothesis**: Test if training on a more diverse dataset (mixing SDSS and DECaLS surveys) improves model generalization to a completely unseen survey (UKIDSS).
2.  **Compare Models**: Benchmark the performance of the original SDSS-only model against a new model trained on a mixed-survey dataset.
3.  **Standardize Evaluation**: Develop a robust evaluation script that can handle models with different feature sets and perform fair comparisons on common ground.
4.  **Analyze & Document**: Quantify the performance difference and document the findings in the project `README.md`.

### Implementation Steps

#### Step 2.1: Identify Models and Feature Mismatch
-   **Models Identified**:
    -   `SDSS-only`: `/pscratch/sd/s/sihany/galaxy-sommelier-data/models/best_model.pt` (74 features)
    -   `Mixed-Survey`: `/pscratch/sd/s/sihany/galaxy-sommelier-data/models/mixed/best_model.pt` (52 features)
-   **Problem**: The two models predict a different number of morphological features. A direct comparison is not possible.
-   **Solution**: The comparison must be done on the subset of features that are common to both models AND have ground-truth labels in the UKIDSS test dataset.

#### Step 2.2: Develop a Unified Comparison Script
-   **File Created**: `scripts/compare_models.py`
-   **Functionality**:
    -   **Dual Mode**: Can run in a 2-model "comparison" mode or a 1-model "benchmark" mode.
    -   **Argument Parsing**: Uses `argparse` to accept model paths, configs, and feature counts from the command line.
    -   **Automated Feature Matching**: Intelligently finds the intersection of features between the model predictions and the ground-truth labels for a fair comparison.
    -   **Metric Calculation**: Computes overall metrics (Correlation, RÂ², MAE, MSE) and detailed per-task correlations.
-   **Outcome**: This script replaces the older `simple_benchmark.py` and `ood_evaluation.py` scripts by combining their functionality into a more powerful and flexible tool.

#### Step 2.3: Execute Comparison and Analyze Results
-   **Execution**: The `compare_models.py` script was run on the full UKIDSS dataset.
-   **Findings**:
    -   The Mixed-Survey model demonstrated superior performance across all metrics.
    -   Overall correlation improved from **0.819** (SDSS-only) to **0.857** (Mixed-Survey).
    -   The largest gains were seen in the classification of bar and spiral features.
-   **Conclusion**: The initial hypothesis was confirmed. Training on more diverse data significantly improves OOD generalization.

#### Step 2.4: Cleanup and Documentation
-   **Code Cleanup**: Removed 5 redundant and outdated scripts from the `scripts` directory.
-   **Documentation**:
    -   Updated the `README.md` title and overview to reflect the project's focus on OOD generalization.
    -   Added a new section to the `README.md` detailing the model comparison, the final results table, and a new plot visualizing the per-task performance gains.
    -   This `plan.txt` file was updated to reflect the completion of Phase 2.

---

## Phase 3: Multi-Survey Domain Adaptation (Week 3-6) ðŸš§ IN PROGRESS

### Key Objectives
1. **Collect multi-survey Galaxy Zoo datasets** (SDSS, DES, DECaLS, HST, UKIDSS)
2. **Quantify domain shift** between different astronomical surveys
3. **Develop survey-agnostic training strategy** using mixed datasets
4. **Create universal morphology classifier** that generalizes across surveys
5. **Establish rigorous evaluation framework** for cross-survey performance

### Scientific Motivation
Current galaxy morphology classifiers suffer from domain shift when applied to different surveys due to:
- Different point spread functions (PSF)
- Varying image depths and signal-to-noise ratios
- Different filter systems and wavelength coverage
- Distinct image processing pipelines

**Solution**: Train a single model on mixed survey data to learn survey-invariant morphological features.

### Implementation Strategy

#### Step 2.1: Multi-Survey Data Collection
```python
# Target Datasets:
datasets = {
    'sdss': {
        'source': 'Galaxy Zoo 2',
        'n_galaxies': '~200,000',
        'resolution': '0.4"/pixel',
        'quality': 'ground-based',
        'status': 'COMPLETE'
    },
    'des': {
        'source': 'Galaxy Zoo DECaLS', 
        'n_galaxies': '~50,000',
        'resolution': '0.262"/pixel',
        'quality': 'better seeing',
        'status': 'PLANNED'
    },
    'decals': {
        'source': 'Galaxy Zoo DECaLS',
        'n_galaxies': '~100,000', 
        'resolution': '0.262"/pixel',
        'quality': 'wide-field',
        'status': 'PLANNED'
    },
    'hst': {
        'source': 'Galaxy Zoo Hubble',
        'n_galaxies': '~15,000',
        'resolution': '0.05"/pixel',
        'quality': 'space-based',
        'status': 'PLANNED'
    },
            'ukidss': {
            'source': 'Galaxy Zoo UKIDSS',
        'n_galaxies': '~10,000',
        'resolution': '0.1"/pixel', 
        'quality': 'space-based',
        'status': 'HELD-OUT TEST SET'
    }
}
```

#### Step 2.2: Domain Shift Analysis
**Experimental Design:**
```python
# Test all pairwise domain shifts
domain_shift_experiments = [
    ('sdss', 'des'),      # Ground â†’ Ground (small shift expected)
    ('sdss', 'hst'),      # Ground â†’ Space (large shift expected)  
                ('sdss', 'ukidss'),   # Ground â†’ Space (largest shift expected)
            ('hst', 'ukidss'),    # Space â†’ Space (small shift expected)
]

# Quality-controlled experiments to isolate survey-invariance effects
def degrade_hst_to_ground_quality(hst_image):
    """Artificially degrade HST to match ground-based quality"""
    # Convolve with seeing disk to match SDSS resolution
    seeing_psf = create_gaussian_psf(fwhm=1.5)  # Typical ground seeing
    degraded = convolve(hst_image, seeing_psf)
    
    # Add realistic noise to match SDSS SNR
    degraded = add_poisson_noise(degraded, snr_target=10)
    
    # Resample to ground resolution
    degraded = downsample(degraded, factor=8)  # 0.05" â†’ 0.4"
    
    return degraded

# Disentangle survey diversity from data quality effects
quality_controlled_experiments = [
    # Baseline: single survey, original quality
    {'sdss': 30000, 'test': 'ukidss'},
    
    # Multi-survey, original quality  
    {'sdss': 15000, 'hst': 15000, 'test': 'ukidss'},
    
    # Multi-survey, quality-matched (degraded HST)
    {'sdss': 15000, 'hst_degraded': 15000, 'test': 'ukidss'},
    
    # Control: degraded HST only
    {'hst_degraded': 30000, 'test': 'ukidss'}
]

# Key insight: If degraded HST images (matched to SDSS quality) still improve 
# cross-survey generalization, we can conclude that survey diversity itself - 
# not image quality - drives the performance gains.

# Progressive multi-survey training
progressive_experiments = [
    {
        'name': 'sdss_only',
        'train_surveys': ['sdss'],
        'test_survey': 'ukidss',
        'expected_performance': 'r ~ 0.4-0.6'
    },
    {
        'name': 'sdss_des',
        'train_surveys': ['sdss', 'des'],
        'test_survey': 'ukidss', 
        'expected_performance': 'r ~ 0.5-0.7'
    },
    {
        'name': 'sdss_des_hst',
        'train_surveys': ['sdss', 'des', 'hst'],
        'test_survey': 'ukidss',
        'expected_performance': 'r ~ 0.7-0.8'
    }
]
```

#### Step 2.3: Multi-Survey Training Strategy
**Dataset Weighting:**
```python
# Quality-weighted sampling for robust learning
survey_weights = {
    'hst': 2.5,      # Highest quality â†’ higher weight
    'ukidss': 2.0,   # Space-based (held-out)
    'des': 1.3,      # Good seeing
    'decals': 1.1,   # Moderate quality
    'sdss': 1.0      # Baseline
}

# Multi-stage training approach:
# Stage 1: Quality-weighted pre-training (10 epochs)
#   - Higher weight for HST/UKIDSS
#   - Learn high-quality morphological features
# Stage 2: Balanced fine-tuning (15 epochs)  
#   - Equal representation across surveys
#   - Learn survey-invariant features
```

#### Step 2.4: Cross-Survey Evaluation Framework
```python
class CrossSurveyEvaluator:
    def __init__(self, surveys, held_out_survey='ukidss'):
        self.surveys = surveys
        self.held_out = held_out_survey
        
    def evaluate_domain_shift(self):
        """Quantify domain shift between all survey pairs"""
        shift_matrix = {}
        for train_survey in self.surveys:
            for test_survey in self.surveys:
                if train_survey != test_survey:
                    model = self.train_single_survey(train_survey)
                    performance = self.evaluate_model(model, test_survey)
                    shift_matrix[(train_survey, test_survey)] = performance
        return shift_matrix
    
    def evaluate_mixed_training(self):
        """Test progressive multi-survey training"""
        results = {}
        
        # Progressive experiments
        for experiment in self.progressive_experiments:
            train_data = self.combine_surveys(experiment['train_surveys'])
            model = self.train_mixed_model(train_data)
            
            # Test on held-out survey
            held_out_perf = self.evaluate_model(model, self.held_out)
            results[experiment['name']] = held_out_perf
            
        return results
```

### Expected Outcomes
1. **Quantified domain shift**: Clear measurement of performance degradation across surveys
2. **Optimal training strategy**: Evidence for which surveys to include in training
3. **Universal classifier**: Single model that works across all major surveys
4. **Benchmark dataset**: Standardized evaluation for cross-survey morphology studies

### Deliverables for Phase 2
- [ ] Multi-survey dataset collection and preprocessing
- [ ] Domain shift quantification across all survey pairs  
- [ ] Mixed training implementation with quality weighting
- [ ] Cross-survey evaluation framework
- [ ] Universal morphology classifier achieving >0.8 correlation on held-out UKIDSS data

---

## Phase 3: Legacy Phase 2 (Out-of-Distribution Evaluation) (Week 7-8)

### Key Objectives
1. **Acquire DESI Legacy Imaging data** for cross-survey validation
2. **Create matched galaxy catalogs** between SDSS, DESI, and HST
3. **Quantify distribution shifts** between surveys
4. **Build comprehensive test sets** for OOD evaluation

### Implementation Steps

#### Step 2.1: DESI Legacy Imaging Data Collection
```python
# scripts/download_desi_data.py
import requests
from astropy.table import Table
from astropy.coordinates import SkyCoord
import astropy.units as u
from PIL import Image
import io

class DESILegacyDownloader:
    def __init__(self, scratch_dir='/pscratch/sd/s/sihany/galaxy-sommelier-data'):
        self.data_dir = os.path.join(scratch_dir, 'desi')
        self.base_url = "https://www.legacysurvey.org"
        os.makedirs(self.data_dir, exist_ok=True)
        
    def download_cutout(self, ra, dec, size=256, pixscale=0.262, bands='grz'):
        """Download DESI Legacy Imaging cutout"""
        # Construct URL for cutout service
        params = {
            'ra': ra,
            'dec': dec,
            'size': size,
            'layer': 'ls-dr9',
            'pixscale': pixscale,
            'bands': bands
        }
        
        url = f"{self.base_url}/viewer/cutout.jpg"
        response = requests.get(url, params=params)
        
        if response.status_code == 200:
            return Image.open(io.BytesIO(response.content))
        else:
            raise Exception(f"Failed to download cutout: {response.status_code}")
    
    def download_fits_cutout(self, ra, dec, size=256, bands=['g', 'r', 'z']):
        """Download FITS format cutouts for each band"""
        cutouts = {}
        
        for band in bands:
            params = {
                'ra': ra,
                'dec': dec,
                'size': size,
                'layer': 'ls-dr9',
                'pixscale': 0.262,
                'bands': band,
                'format': 'fits'
            }
            
            url = f"{self.base_url}/viewer/cutout.fits"
            response = requests.get(url, params=params)
            
            if response.status_code == 200:
                # Save FITS file
                filename = f"desi_{ra:.5f}_{dec:.5f}_{band}.fits"
                filepath = os.path.join(self.data_dir, filename)
                with open(filepath, 'wb') as f:
                    f.write(response.content)
                cutouts[band] = filepath
                
        return cutouts
    
    def batch_download_from_catalog(self, galaxy_catalog, n_jobs=4):
        """Download DESI images for a catalog of galaxies"""
        from joblib import Parallel, delayed
        
        def download_single(row):
            try:
                self.download_fits_cutout(row['ra'], row['dec'])
                return True
            except Exception as e:
                print(f"Error downloading {row['ra']}, {row['dec']}: {e}")
                return False
        
        # Parallel download
        results = Parallel(n_jobs=n_jobs)(
            delayed(download_single)(row) for _, row in galaxy_catalog.iterrows()
        )
        
        success_rate = sum(results) / len(results)
        print(f"Successfully downloaded {success_rate:.1%} of galaxies")
```

#### Step 2.2: Cross-Survey Matching
```python
# scripts/create_matched_samples.py
import numpy as np
from astropy.coordinates import SkyCoord, match_coordinates_sky
from astropy.table import Table, join

class CrossSurveyMatcher:
    def __init__(self, scratch_dir='/pscratch/sd/s/sihany/galaxy-sommelier-data'):
        self.scratch_dir = scratch_dir
        self.catalogs = {}
        
    def load_survey_catalogs(self):
        """Load catalogs from different surveys"""
        # Galaxy Zoo (SDSS-based)
        self.catalogs['sdss'] = pd.read_csv(
            os.path.join(self.scratch_dir, 'catalogs/gz2_hart16.csv')
        )
        
        # DESI Legacy Survey catalog
        # You would get this from DESI data releases
        self.catalogs['desi'] = self.load_desi_catalog()
        
        # HST catalog (from various HST galaxy surveys)
        self.catalogs['hst'] = self.load_hst_catalog()
    
    def find_matches(self, ref_survey='sdss', target_survey='desi', 
                     max_sep=2.0*u.arcsec):
        """Find galaxies observed by multiple surveys"""
        ref_cat = self.catalogs[ref_survey]
        target_cat = self.catalogs[target_survey]
        
        # Create coordinate objects
        ref_coords = SkyCoord(
            ra=ref_cat['ra'].values*u.deg, 
            dec=ref_cat['dec'].values*u.deg
        )
        target_coords = SkyCoord(
            ra=target_cat['ra'].values*u.deg, 
            dec=target_cat['dec'].values*u.deg
        )
        
        # Perform matching
        idx, d2d, d3d = match_coordinates_sky(ref_coords, target_coords)
        
        # Select good matches
        good_matches = d2d < max_sep
        
        # Create matched catalog
        matched = pd.DataFrame({
            f'{ref_survey}_idx': np.where(good_matches)[0],
            f'{target_survey}_idx': idx[good_matches],
            'separation_arcsec': d2d[good_matches].arcsec,
            f'{ref_survey}_ra': ref_cat.iloc[good_matches]['ra'].values,
            f'{ref_survey}_dec': ref_cat.iloc[good_matches]['dec'].values,
        })
        
        print(f"Found {len(matched)} matches between {ref_survey} and {target_survey}")
        return matched
    
    def create_triple_matches(self):
        """Find galaxies in all three surveys"""
        # First match SDSS-DESI
        sdss_desi = self.find_matches('sdss', 'desi')
        
        # Then match those with HST
        temp_catalog = pd.DataFrame({
            'ra': sdss_desi['sdss_ra'],
            'dec': sdss_desi['sdss_dec']
        })
        
        # Match with HST
        # ... implementation continues
```

#### Step 2.3: Distribution Shift Analysis
```python
# scripts/analyze_distribution_shifts.py
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import wasserstein_distance, ks_2samp
from skimage.filters import gaussian

class DistributionAnalyzer:
    def __init__(self, data_dir='/pscratch/sd/s/sihany/galaxy-sommelier-data'):
        self.data_dir = data_dir
        self.results = {}
        
    def analyze_survey_characteristics(self, survey_name, sample_size=1000):
        """Comprehensive analysis of survey properties"""
        images = self.load_survey_sample(survey_name, sample_size)
        
        characteristics = {
            'pixel_statistics': self.compute_pixel_stats(images),
            'noise_properties': self.analyze_noise(images),
            'resolution_metrics': self.estimate_resolution(images),
            'color_distributions': self.analyze_colors(images),
            'morphology_coverage': self.analyze_morphology_space(images)
        }
        
        return characteristics
    
    def compute_pixel_stats(self, images):
        """Analyze pixel value distributions"""
        stats = {
            'mean_brightness': np.mean([img.mean() for img in images]),
            'std_brightness': np.std([img.mean() for img in images]),
            'dynamic_range': np.mean([img.max() - img.min() for img in images]),
            'saturation_fraction': np.mean([
                (img > np.percentile(img, 99.5)).sum() / img.size 
                for img in images
            ])
        }
        return stats
    
    def analyze_noise(self, images):
        """Estimate noise characteristics"""
        noise_estimates = []
        
        for img in images:
            # Estimate noise from background regions
            background_mask = img < np.percentile(img, 30)
            if background_mask.sum() > 100:
                noise = np.std(img[background_mask])
                noise_estimates.append(noise)
        
        return {
            'median_noise': np.median(noise_estimates),
            'noise_variation': np.std(noise_estimates),
            'snr_estimates': self.estimate_snr(images, noise_estimates)
        }
    
    def compare_surveys(self, survey1, survey2):
        """Detailed comparison between two surveys"""
        chars1 = self.analyze_survey_characteristics(survey1)
        chars2 = self.analyze_survey_characteristics(survey2)
        
        comparison = {
            'brightness_shift': self.compute_distribution_distance(
                chars1['pixel_statistics'], 
                chars2['pixel_statistics']
            ),
            'resolution_ratio': (
                chars2['resolution_metrics']['estimated_fwhm'] / 
                chars1['resolution_metrics']['estimated_fwhm']
            ),
            'noise_ratio': (
                chars2['noise_properties']['median_noise'] / 
                chars1['noise_properties']['median_noise']
            ),
            'color_space_overlap': self.compute_color_overlap(
                chars1['color_distributions'],
                chars2['color_distributions']
            )
        }
        
        return comparison
    
    def create_diagnostic_plots(self, output_dir):
        """Generate comprehensive diagnostic plots"""
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        
        # Plot 1: Brightness distributions
        self.plot_brightness_comparison(axes[0, 0])
        
        # Plot 2: Resolution comparison
        self.plot_resolution_analysis(axes[0, 1])
        
        # Plot 3: Noise characteristics
        self.plot_noise_comparison(axes[0, 2])
        
        # Plot 4: Color distributions
        self.plot_color_distributions(axes[1, 0])
        
        # Plot 5: Morphology space coverage
        self.plot_morphology_coverage(axes[1, 1])
        
        # Plot 6: Example galaxies
        self.plot_example_comparisons(axes[1, 2])
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'distribution_analysis.png'), dpi=300)
```

### Deliverables for Phase 2
- [ ] DESI Legacy Imaging data pipeline integrated
- [ ] Matched catalog with >1000 galaxies across surveys
- [ ] Comprehensive distribution shift analysis report
- [ ] Visualization dashboard showing survey differences

---

## Phase 3: OOD Testing Framework (Week 5-6)

### Key Objectives
1. **Build comprehensive diagnostic suite** for model evaluation
2. **Implement uncertainty quantification** methods
3. **Create automated testing pipeline** for continuous evaluation
4. **Establish performance baselines** across different domains

### Implementation Steps

#### Step 3.1: Core Diagnostic Framework
```python
# scripts/ood_diagnostics.py
import torch
import numpy as np
from sklearn.metrics import cohen_kappa_score, mean_absolute_error
from scipy.stats import spearmanr
import json

class OODDiagnosticSuite:
    def __init__(self, model, device='cuda', results_dir='./results'):
        self.model = model
        self.device = device
        self.results_dir = results_dir
        self.model.eval()
        
        # Store all diagnostic results
        self.results = {
            'timestamp': datetime.now().isoformat(),
            'model_version': self.get_model_version(),
            'diagnostics': {}
        }
        
    def run_full_diagnostic(self, test_datasets):
        """Run complete diagnostic suite on all test datasets"""
        for dataset_name, dataset in test_datasets.items():
            print(f"\nRunning diagnostics on {dataset_name}...")
            
            self.results['diagnostics'][dataset_name] = {
                'resolution_robustness': self.test_resolution_robustness(dataset),
                'noise_robustness': self.test_noise_robustness(dataset),
                'color_consistency': self.test_color_consistency(dataset),
                'feature_stability': self.test_feature_stability(dataset),
                'uncertainty_calibration': self.test_uncertainty_calibration(dataset),
                'cross_survey_agreement': self.test_cross_survey_agreement(dataset)
            }
        
        # Save results
        self.save_results()
        
        # Generate report
        self.generate_report()
        
    def test_resolution_robustness(self, dataset, scales=[0.5, 0.75, 1.0, 1.5, 2.0]):
        """Test model performance across different resolutions"""
        results = {'scales': scales, 'metrics': {}}
        
        # Get baseline predictions
        baseline_preds = []
        with torch.no_grad():
            for batch in dataset:
                preds = self.model(batch['image'].to(self.device))
                baseline_preds.append(preds.cpu())
        baseline_preds = torch.cat(baseline_preds)
        
        # Test each scale
        for scale in scales:
            scaled_preds = []
            
            with torch.no_grad():
                for batch in dataset:
                    # Apply resolution scaling
                    scaled_images = self.scale_resolution(batch['image'], scale)
                    preds = self.model(scaled_images.to(self.device))
                    scaled_preds.append(preds.cpu())
            
            scaled_preds = torch.cat(scaled_preds)
            
            # Compute metrics
            results['metrics'][f'scale_{scale}'] = {
                'mae': mean_absolute_error(
                    baseline_preds.numpy(), 
                    scaled_preds.numpy()
                ),
                'correlation': np.corrcoef(
                    baseline_preds.numpy().flatten(),
                    scaled_preds.numpy().flatten()
                )[0, 1],
                'feature_consistency': self.compute_feature_consistency(
                    baseline_preds, scaled_preds
                )
            }
        
        return results
    
    def test_uncertainty_calibration(self, dataset, n_bins=10):
        """Test if model uncertainties are well-calibrated"""
        predictions = []
        uncertainties = []
        targets = []
        
        # Enable dropout for uncertainty estimation
        self.enable_dropout()
        
        with torch.no_grad():
            for batch in dataset:
                # Multiple forward passes
                batch_preds = []
                for _ in range(10):
                    pred = self.model(batch['image'].to(self.device))
                    batch_preds.append(pred)
                
                batch_preds = torch.stack(batch_preds)
                mean_pred = batch_preds.mean(dim=0)
                uncertainty = batch_preds.std(dim=0)
                
                predictions.append(mean_pred.cpu())
                uncertainties.append(uncertainty.cpu())
                targets.append(batch['labels'])
        
        predictions = torch.cat(predictions)
        uncertainties = torch.cat(uncertainties)
        targets = torch.cat(targets)
        
        # Compute calibration metrics
        calibration_results = self.compute_calibration_metrics(
            predictions, uncertainties, targets, n_bins
        )
        
        return calibration_results
    
    def test_cross_survey_agreement(self, matched_dataset):
        """Test consistency across matched galaxies from different surveys"""
        agreements = {
            'sdss_desi': [],
            'sdss_hst': [],
            'desi_hst': []
        }
        
        with torch.no_grad():
            for batch in matched_dataset:
                # Get predictions for each survey
                preds = {}
                for survey in ['sdss', 'desi', 'hst']:
                    if survey in batch:
                        pred = self.model(batch[survey].to(self.device))
                        preds[survey] = pred.cpu()
                
                # Compute pairwise agreements
                if 'sdss' in preds and 'desi' in preds:
                    agreements['sdss_desi'].append(
                        self.compute_agreement(preds['sdss'], preds['desi'])
                    )
                
                if 'sdss' in preds and 'hst' in preds:
                    agreements['sdss_hst'].append(
                        self.compute_agreement(preds['sdss'], preds['hst'])
                    )
                
                if 'desi' in preds and 'hst' in preds:
                    agreements['desi_hst'].append(
                        self.compute_agreement(preds['desi'], preds['hst'])
                    )
        
        # Aggregate results
        results = {}
        for pair, scores in agreements.items():
            if scores:
                results[pair] = {
                    'mean_agreement': np.mean(scores),
                    'std_agreement': np.std(scores),
                    'min_agreement': np.min(scores),
                    'max_agreement': np.max(scores)
                }
        
        return results
```

#### Step 3.2: Visualization Dashboard
```python
# scripts/create_diagnostic_dashboard.py
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px

class DiagnosticDashboard:
    def __init__(self, diagnostic_results):
        self.results = diagnostic_results
        
    def create_interactive_dashboard(self, output_path):
        """Create comprehensive interactive dashboard"""
        # Create subplots
        fig = make_subplots(
            rows=3, cols=3,
            subplot_titles=(
                'Resolution Robustness', 'Noise Robustness', 'Feature Stability',
                'Uncertainty Calibration', 'Cross-Survey Agreement', 'Error Distribution',
                'Performance by Morphology', 'Failure Analysis', 'Confidence Distribution'
            ),
            specs=[
                [{'type': 'scatter'}, {'type': 'scatter'}, {'type': 'bar'}],
                [{'type': 'scatter'}, {'type': 'bar'}, {'type': 'histogram'}],
                [{'type': 'bar'}, {'type': 'scatter'}, {'type': 'box'}]
            ]
        )
        
        # Add plots
        self.add_resolution_plot(fig, row=1, col=1)
        self.add_noise_plot(fig, row=1, col=2)
        self.add_feature_stability_plot(fig, row=1, col=3)
        self.add_calibration_plot(fig, row=2, col=1)
        self.add_agreement_plot(fig, row=2, col=2)
        self.add_error_distribution(fig, row=2, col=3)
        self.add_morphology_performance(fig, row=3, col=1)
        self.add_failure_analysis(fig, row=3, col=2)
        self.add_confidence_distribution(fig, row=3, col=3)
        
        # Update layout
        fig.update_layout(
            height=1200,
            showlegend=True,
            title_text="Galaxy Sommelier OOD Diagnostic Dashboard",
            title_font_size=20
        )
        
        # Save dashboard
        fig.write_html(output_path)
        
    def add_resolution_plot(self, fig, row, col):
        """Plot resolution robustness results"""
        for survey in ['sdss', 'desi', 'hst']:
            if survey in self.results['diagnostics']:
                res_data = self.results['diagnostics'][survey]['resolution_robustness']
                
                scales = res_data['scales']
                maes = [res_data['metrics'][f'scale_{s}']['mae'] for s in scales]
                
                fig.add_trace(
                    go.Scatter(
                        x=scales,
                        y=maes,
                        mode='lines+markers',
                        name=survey.upper(),
                        line=dict(width=2),
                        marker=dict(size=8)
                    ),
                    row=row, col=col
                )
        
        fig.update_xaxes(title_text="Resolution Scale Factor", row=row, col=col)
        fig.update_yaxes(title_text="Mean Absolute Error", row=row, col=col)
```

#### Step 3.3: Automated Testing Pipeline
```python
# scripts/automated_testing.py
import schedule
import time
from pathlib import Path

class AutomatedTestingPipeline:
    def __init__(self, model_dir, test_data_dir, results_dir):
        self.model_dir = Path(model_dir)
        self.test_data_dir = Path(test_data_dir)
        self.results_dir = Path(results_dir)
        
    def run_nightly_tests(self):
        """Run comprehensive tests on latest model"""
        # Find latest model
        latest_model = self.find_latest_model()
        
        # Load model
        model = self.load_model(latest_model)
        
        # Initialize diagnostic suite
        diagnostics = OODDiagnosticSuite(model, results_dir=self.results_dir)
        
        # Load test datasets
        test_datasets = {
            'sdss_test': self.load_dataset('sdss'),
            'desi_test': self.load_dataset('desi'),
            'hst_test': self.load_dataset('hst'),
            'matched_test': self.load_matched_dataset()
        }
        
        # Run diagnostics
        diagnostics.run_full_diagnostic(test_datasets)
        
        # Check for regressions
        self.check_for_regressions(diagnostics.results)
        
        # Send notification
        self.send_notification(diagnostics.results)
    
    def check_for_regressions(self, current_results):
        """Compare with previous results to detect regressions"""
        previous_results = self.load_previous_results()
        
        if previous_results:
            regressions = []
            
            for survey in current_results['diagnostics']:
                for metric in current_results['diagnostics'][survey]:
                    current_value = self.extract_metric_value(
                        current_results['diagnostics'][survey][metric]
                    )
                    previous_value = self.extract_metric_value(
                        previous_results['diagnostics'][survey][metric]
                    )
                    
                    # Check for significant degradation
                    if self.is_regression(current_value, previous_value, metric):
                        regressions.append({
                            'survey': survey,
                            'metric': metric,
                            'current': current_value,
                            'previous': previous_value,
                            'change': (current_value - previous_value) / previous_value
                        })
            
            if regressions:
                self.alert_regressions(regressions)
    
    def schedule_tests(self):
        """Schedule automated testing"""
        # Run tests every night at 2 AM
        schedule.every().day.at("02:00").do(self.run_nightly_tests)
        
        # Run quick tests every 6 hours
        schedule.every(6).hours.do(self.run_quick_tests)
        
        print("Automated testing scheduled. Press Ctrl+C to stop.")
        
        while True:
            schedule.run_pending()
            time.sleep(60)
```

### Deliverables for Phase 3
- [ ] Complete OOD diagnostic suite with 10+ test categories
- [ ] Interactive visualization dashboard
- [ ] Automated testing pipeline with regression detection
- [ ] Baseline performance metrics across all surveys

---

## Phase 4: Domain Adaptation Implementation (Week 7-8)

### Key Objectives
1. **Implement advanced augmentation** strategies for domain robustness
2. **Deploy domain adversarial training** to align feature distributions
3. **Apply self-supervised learning** on unlabeled target domain data
4. **Achieve <15% performance drop** on OOD data

### Implementation Steps

#### Step 4.1: Survey-Aware Data Augmentation
```python
# scripts/advanced_augmentation.py
import cv2
import numpy as np
from scipy.ndimage import gaussian_filter
from astropy.convolution import Gaussian2DKernel, convolve

class SurveyAwareAugmentation:
    def __init__(self, source_survey='sdss'):
        self.source_survey = source_survey
        
        # Survey characteristics
        self.survey_params = {
            'sdss': {
                'pixel_scale': 0.396,  # arcsec/pixel
                'typical_seeing': 1.4,  # arcsec
                'typical_noise': 0.02,
                'bands': ['u', 'g', 'r', 'i', 'z']
            },
            'desi': {
                'pixel_scale': 0.262,
                'typical_seeing': 1.0,
                'typical_noise': 0.01,
                'bands': ['g', 'r', 'i', 'z']
            },
            'hst': {
                'pixel_scale': 0.05,
                'typical_seeing': 0.1,
                'typical_noise': 0.001,
                'bands': ['F475W', 'F606W', 'F814W']
            }
        }
        
    def __call__(self, image, target_survey=None):
        """Apply survey-specific augmentations"""
        if target_survey is None:
            # Randomly choose a target survey
            target_survey = np.random.choice(['sdss', 'desi', 'hst'])
        
        # Apply transformations to simulate target survey
        image = self.adjust_resolution(image, target_survey)
        image = self.apply_psf(image, target_survey)
        image = self.add_survey_noise(image, target_survey)
        image = self.adjust_dynamic_range(image, target_survey)
        
        return image
    
    def adjust_resolution(self, image, target_survey):
        """Adjust image resolution to match target survey"""
        source_scale = self.survey_params[self.source_survey]['pixel_scale']
        target_scale = self.survey_params[target_survey]['pixel_scale']
        
        scale_factor = source_scale / target_scale
        
        if scale_factor > 1:  # Downsample
            # Apply anti-aliasing before downsampling
            sigma = scale_factor / 2
            image = gaussian_filter(image, sigma=sigma)
            
            # Downsample
            new_size = (int(image.shape[1] / scale_factor), 
                       int(image.shape[0] / scale_factor))
            image = cv2.resize(image, new_size, interpolation=cv2.INTER_AREA)
            
            # Resize back to original size
            image = cv2.resize(image, (image.shape[1], image.shape[0]), 
                             interpolation=cv2.INTER_CUBIC)
        
        elif scale_factor < 1:  # Upsample (simulating higher resolution)
            # This is trickier - we can't create information
            # Add slight sharpening to simulate higher resolution
            kernel = np.array([[-1,-1,-1], 
                              [-1, 9,-1], 
                              [-1,-1,-1]]) * 0.1
            image = cv2.filter2D(image, -1, kernel)
        
        return image
    
    def apply_psf(self, image, target_survey):
        """Apply point spread function to simulate seeing"""
        source_seeing = self.survey_params[self.source_survey]['typical_seeing']
        target_seeing = self.survey_params[target_survey]['typical_seeing']
        
        # Calculate additional blurring needed
        source_sigma = source_seeing / (2.355 * self.survey_params[self.source_survey]['pixel_scale'])
        target_sigma = target_seeing / (2.355 * self.survey_params[target_survey]['pixel_scale'])
        
        if target_sigma > source_sigma:
            # Need to add blurring
            additional_sigma = np.sqrt(target_sigma**2 - source_sigma**2)
            kernel = Gaussian2DKernel(additional_sigma)
            image = convolve(image, kernel)
        
        return image
```

#### Step 4.2: Domain Adversarial Network
```python
# scripts/domain_adversarial_training.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Function

class GradientReversalFunction(Function):
    @staticmethod
    def forward(ctx, x, lambda_):
        ctx.lambda_ = lambda_
        return x.clone()
    
    @staticmethod
    def backward(ctx, grads):
        lambda_ = ctx.lambda_
        lambda_ = grads.new_tensor(lambda_)
        dx = -lambda_ * grads
        return dx, None

class GradientReversal(nn.Module):
    def __init__(self, lambda_=1):
        super().__init__()
        self.lambda_ = lambda_
        
    def forward(self, x):
        return GradientReversalFunction.apply(x, self.lambda_)

class DomainAdversarialGalaxySommelier(nn.Module):
    def __init__(self, base_model, num_domains=3):
        super().__init__()
        
        # Feature extractor from base model
        self.feature_extractor = base_model.dinov2
        
        # Task-specific head
        self.morphology_classifier = base_model.morphology_head
        
        # Domain discriminator with gradient reversal
        hidden_size = base_model.dinov2.config.hidden_size
        self.gradient_reversal = GradientReversal()
        self.domain_discriminator = nn.Sequential(
            nn.Linear(hidden_size, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, num_domains)
        )
        
    def forward(self, pixel_values, alpha=1.0):
        # Extract features
        features = self.feature_extractor(pixel_values=pixel_values).pooler_output
        
        # Task predictions
        morphology_preds = self.morphology_classifier(features)
        
        # Domain predictions with gradient reversal
        self.gradient_reversal.lambda_ = alpha
        reversed_features = self.gradient_reversal(features)
        domain_preds = self.domain_discriminator(reversed_features)
        
        return morphology_preds, domain_preds

class DomainAdversarialTrainer:
    def __init__(self, model, config):
        self.model = model
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
    def train_epoch(self, source_loader, target_loader, optimizer, epoch):
        self.model.train()
        
        total_task_loss = 0
        total_domain_loss = 0
        
        # Create iterators
        target_iter = iter(target_loader)
        
        for batch_idx, source_batch in enumerate(source_loader):
            # Get target batch
            try:
                target_batch = next(target_iter)
            except StopIteration:
                target_iter = iter(target_loader)
                target_batch = next(target_iter)
            
            # Move to device
            source_images = source_batch['images'].to(self.device)
            source_labels = source_batch['labels'].to(self.device)
            target_images = target_batch['images'].to(self.device)
            
            # Create domain labels
            source_domain = torch.zeros(len(source_images), device=self.device, dtype=torch.long)
            target_domain = torch.ones(len(target_images), device=self.device, dtype=torch.long)
            
            # Adaptation parameter (gradually increase)
            p = float(batch_idx + epoch * len(source_loader)) / (self.config['num_epochs'] * len(source_loader))
            alpha = 2. / (1. + np.exp(-10 * p)) - 1
            
            # Forward pass
            source_morphology, source_domain_pred = self.model(source_images, alpha)
            target_morphology, target_domain_pred = self.model(target_images, alpha)
            
            # Task loss (only on source)
            task_loss = F.mse_loss(source_morphology, source_labels)
            
            # Domain loss
            domain_loss = F.cross_entropy(
                torch.cat([source_domain_pred, target_domain_pred]),
                torch.cat([source_domain, target_domain])
            )
            
            # Combined loss
            total_loss = task_loss + self.config['domain_weight'] * domain_loss
            
            # Backward pass
            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()
            
            total_task_loss += task_loss.item()
            total_domain_loss += domain_loss.item()
        
        return {
            'task_loss': total_task_loss / len(source_loader),
            'domain_loss': total_domain_loss / len(source_loader)
        }
```

#### Step 4.3: Self-Supervised Adaptation
```python
# scripts/self_supervised_adaptation.py
import torch
import torch.nn as nn
import torch.nn.functional as F

class SimCLRProjectionHead(nn.Module):
    def __init__(self, input_dim, hidden_dim=256, output_dim=128):
        super().__init__()
        self.projection = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )
        
    def forward(self, x):
        return self.projection(x)

class SelfSupervisedAdapter:
    def __init__(self, base_model, config):
        self.base_model = base_model
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Add projection head for contrastive learning
        hidden_size = base_model.dinov2.config.hidden_size
        self.projection_head = SimCLRProjectionHead(hidden_size).to(self.device)
        
    def create_augmented_pairs(self, images):
        """Create two differently augmented versions of each image"""
        augmentation = SurveyAwareAugmentation()
        
        # Apply different augmentations
        aug1 = torch.stack([
            augmentation(img, target_survey='desi') for img in images
        ])
        
        aug2 = torch.stack([
            augmentation(img, target_survey='hst') for img in images
        ])
        
        return aug1, aug2
    
    def info_nce_loss(self, features1, features2, temperature=0.5):
        """Compute InfoNCE loss for contrastive learning"""
        batch_size = features1.shape[0]
        
        # Normalize features
        features1 = F.normalize(features1, dim=1)
        features2 = F.normalize(features2, dim=1)
        
        # Compute similarity matrix
        features = torch.cat([features1, features2], dim=0)
        similarity_matrix = torch.matmul(features, features.T) / temperature
        
        # Create labels
        labels = torch.cat([torch.arange(batch_size) for _ in range(2)], dim=0)
        labels = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()
        labels = labels.to(self.device)
        
        # Mask out self-similarity
        mask = torch.eye(labels.shape[0], dtype=torch.bool).to(self.device)
        labels = labels[~mask].view(labels.shape[0], -1)
        similarity_matrix = similarity_matrix[~mask].view(similarity_matrix.shape[0], -1)
        
        # Select positives
        positives = similarity_matrix[labels.bool()].view(labels.shape[0], -1)
        
        # Select negatives
        negatives = similarity_matrix[~labels.bool()].view(similarity_matrix.shape[0], -1)
        
        # Compute loss
        logits = torch.cat([positives, negatives], dim=1)
        labels = torch.zeros(logits.shape[0], dtype=torch.long).to(self.device)
        
        loss = F.cross_entropy(logits, labels)
        return loss
    
    def adapt_on_target_domain(self, target_loader, num_epochs=10):
        """Perform self-supervised adaptation on unlabeled target data"""
        # Combine feature extractor and projection head
        optimizer = torch.optim.Adam(
            list(self.base_model.dinov2.parameters()) + 
            list(self.projection_head.parameters()),
            lr=self.config['ssl_learning_rate']
        )
        
        for epoch in range(num_epochs):
            total_loss = 0
            
            for batch in target_loader:
                images = batch['images'].to(self.device)
                
                # Create augmented pairs
                images1, images2 = self.create_augmented_pairs(images)
                
                # Extract features
                with torch.no_grad():
                    features1 = self.base_model.dinov2(pixel_values=images1).pooler_output
                    features2 = self.base_model.dinov2(pixel_values=images2).pooler_output
                
                # Project features
                z1 = self.projection_head(features1)
                z2 = self.projection_head(features2)
                
                # Compute contrastive loss
                loss = self.info_nce_loss(z1, z2, temperature=self.config['temperature'])
                
                # Backward pass
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
            
            print(f"SSL Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(target_loader):.4f}")
```

### Deliverables for Phase 4
- [ ] Survey-aware augmentation pipeline
- [ ] Domain adversarial training implementation
- [ ] Self-supervised adaptation on target domains
- [ ] Model achieving <15% OOD performance drop

---

## Phase 5: Continuous Improvement Loop (Week 9-10)

### Key Objectives
1. **Implement active learning** for efficient labeling
2. **Set up model versioning** and A/B testing infrastructure
3. **Create production pipeline** for deployment
4. **Establish monitoring** for continuous improvement

### Implementation Steps

#### Step 5.1: Active Learning System
```python
# scripts/active_learning_system.py
import numpy as np
from scipy.stats import entropy
from sklearn.cluster import KMeans

class ActiveLearningSelector:
    def __init__(self, model, strategy='uncertainty'):
        self.model = model
        self.strategy = strategy
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
    def select_samples(self, unlabeled_pool, budget=100):
        """Select most informative samples for labeling"""
        if self.strategy == 'uncertainty':
            return self.uncertainty_sampling(unlabeled_pool, budget)
        elif self.strategy == 'diversity':
            return self.diversity_sampling(unlabeled_pool, budget)
        elif self.strategy == 'hybrid':
            return self.hybrid_sampling(unlabeled_pool, budget)
        else:
            raise ValueError(f"Unknown strategy: {self.strategy}")
    
    def uncertainty_sampling(self, unlabeled_pool, budget):
        """Select samples with highest prediction uncertainty"""
        uncertainties = []
        indices = []
        
        self.model.eval()
        with torch.no_grad():
            for idx, sample in enumerate(unlabeled_pool):
                # Multiple forward passes with dropout
                predictions = []
                for _ in range(10):
                    pred = self.model(sample['image'].unsqueeze(0).to(self.device))
                    predictions.append(pred.cpu().numpy())
                
                predictions = np.array(predictions)
                
                # Compute uncertainty (entropy of predictions)
                mean_pred = predictions.mean(axis=0)
                uncertainty = entropy(mean_pred.flatten())
                
                uncertainties.append(uncertainty)
                indices.append(idx)
        
        # Select top-k uncertain samples
        uncertainties = np.array(uncertainties)
        selected_indices = np.argsort(uncertainties)[-budget:]
        
        return [indices[i] for i in selected_indices]
    
    def diversity_sampling(self, unlabeled_pool, budget):
        """Select diverse samples using clustering in feature space"""
        features = []
        indices = []
        
        self.model.eval()
        with torch.no_grad():
            for idx, sample in enumerate(unlabeled_pool):
                # Extract features
                _, feat = self.model(
                    sample['image'].unsqueeze(0).to(self.device), 
                    return_features=True
                )
                features.append(feat.cpu().numpy().flatten())
                indices.append(idx)
        
        features = np.array(features)
        
        # Cluster features
        kmeans = KMeans(n_clusters=budget, random_state=42)
        cluster_labels = kmeans.fit_predict(features)
        
        # Select one sample per cluster (closest to centroid)
        selected_indices = []
        for cluster_id in range(budget):
            cluster_mask = cluster_labels == cluster_id
            cluster_features = features[cluster_mask]
            cluster_indices = np.array(indices)[cluster_mask]
            
            # Find closest to centroid
            centroid = kmeans.cluster_centers_[cluster_id]
            distances = np.linalg.norm(cluster_features - centroid, axis=1)
            closest_idx = np.argmin(distances)
            
            selected_indices.append(cluster_indices[closest_idx])
        
        return selected_indices
    
    def hybrid_sampling(self, unlabeled_pool, budget):
        """Combine uncertainty and diversity sampling"""
        # First, select 2x budget using uncertainty
        uncertain_indices = self.uncertainty_sampling(unlabeled_pool, budget * 2)
        
        # Then, select final budget using diversity from uncertain samples
        uncertain_pool = [unlabeled_pool[i] for i in uncertain_indices]
        diverse_indices = self.diversity_sampling(uncertain_pool, budget)
        
        # Map back to original indices
        final_indices = [uncertain_indices[i] for i in diverse_indices]
        
        return final_indices
```

#### Step 5.2: Model Registry and Versioning
```python
# scripts/model_registry.py
import json
import hashlib
from datetime import datetime
import wandb

class ModelRegistry:
    def __init__(self, registry_path='/pscratch/sd/s/sihany/galaxy-sommelier-data/model_registry'):
        self.registry_path = Path(registry_path)
        self.registry_path.mkdir(exist_ok=True)
        self.registry_file = self.registry_path / 'registry.json'
        
        # Load existing registry
        if self.registry_file.exists():
            with open(self.registry_file, 'r') as f:
                self.registry = json.load(f)
        else:
            self.registry = {'models': {}}
    
    def register_model(self, model, metrics, metadata):
        """Register a new model version"""
        # Generate model hash
        model_hash = self.compute_model_hash(model)
        
        # Create version ID
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        version_id = f"v_{timestamp}_{model_hash[:8]}"
        
        # Save model
        model_path = self.registry_path / f"{version_id}.pt"
        torch.save({
            'model_state_dict': model.state_dict(),
            'config': model.config if hasattr(model, 'config') else {},
            'timestamp': timestamp
        }, model_path)
        
        # Update registry
        self.registry['models'][version_id] = {
            'path': str(model_path),
            'timestamp': timestamp,
            'hash': model_hash,
            'metrics': metrics,
            'metadata': metadata,
            'status': 'staged'  # staged, production, archived
        }
        
        # Save registry
        self.save_registry()
        
        # Log to wandb
        wandb.log({
            'model_version': version_id,
            **metrics,
            **metadata
        })
        
        return version_id
    
    def promote_to_production(self, version_id):
        """Promote a model to production"""
        # Demote current production model
        for vid, info in self.registry['models'].items():
            if info['status'] == 'production':
                info['status'] = 'archived'
                info['demoted_at'] = datetime.now().isoformat()
        
        # Promote new model
        self.registry['models'][version_id]['status'] = 'production'
        self.registry['models'][version_id]['promoted_at'] = datetime.now().isoformat()
        
        self.save_registry()
        
        # Create production symlink
        prod_link = self.registry_path / 'production_model.pt'
        if prod_link.exists():
            prod_link.unlink()
        prod_link.symlink_to(self.registry['models'][version_id]['path'])
    
    def compare_models(self, version_a, version_b, test_data):
        """A/B test two model versions"""
        # Load models
        model_a = self.load_model(version_a)
        model_b = self.load_model(version_b)
        
        # Run evaluation
        results_a = self.evaluate_model(model_a, test_data)
        results_b = self.evaluate_model(model_b, test_data)
        
        # Statistical comparison
        from scipy.stats import wilcoxon, ttest_rel
        
        comparison = {
            'version_a': version_a,
            'version_b': version_b,
            'metrics_a': results_a['metrics'],
            'metrics_b': results_b['metrics'],
            'statistical_tests': {}
        }
        
        # Paired tests on per-sample predictions
        for metric in ['mae', 'correlation', 'uncertainty_calibration']:
            scores_a = results_a['per_sample'][metric]
            scores_b = results_b['per_sample'][metric]
            
            # Wilcoxon signed-rank test
            stat, p_value = wilcoxon(scores_a, scores_b)
            
            comparison['statistical_tests'][metric] = {
                'statistic': stat,
                'p_value': p_value,
                'significant': p_value < 0.05,
                'better_model': version_a if np.mean(scores_a) < np.mean(scores_b) else version_b
            }
        
        return comparison
```

#### Step 5.3: Production Deployment Pipeline
```python
# scripts/production_pipeline.py
import torch
from typing import Dict, List, Optional
import logging

class GalaxySommelierProduction:
    def __init__(self, model_registry, config_path='./configs/production.yaml'):
        self.registry = model_registry
        self.config = self.load_config(config_path)
        self.model = None
        self.processor = None
        
        # Setup logging
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # Load production model
        self.load_production_model()
        
    def load_production_model(self):
        """Load the current production model"""
        prod_model_path = self.registry.registry_path / 'production_model.pt'
        
        if not prod_model_path.exists():
            raise RuntimeError("No production model found!")
        
        # Load model
        checkpoint = torch.load(prod_model_path)
        self.model = GalaxySommelier(**checkpoint.get('config', {}))
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.eval()
        
        # Move to appropriate device
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        
        self.logger.info(f"Loaded production model from {prod_model_path}")
    
    def process_galaxy(self, image_path: str, 
                      return_uncertainty: bool = True,
                      return_features: bool = False) -> Dict:
        """Process a single galaxy image"""
        try:
            # Load and preprocess image
            image = self.load_image(image_path)
            
            # Detect survey automatically
            survey = self.detect_survey(image_path)
            
            # Apply appropriate preprocessing
            processed = self.preprocess_for_survey(image, survey)
            
            # Run inference
            with torch.no_grad():
                # Get predictions
                if return_uncertainty:
                    predictions, uncertainty = self.predict_with_uncertainty(processed)
                else:
                    predictions = self.model(processed.unsqueeze(0).to(self.device))
                    uncertainty = None
                
                # Get features if requested
                if return_features:
                    _, features = self.model(processed.unsqueeze(0).to(self.device), 
                                           return_features=True)
                else:
                    features = None
            
            # Package results
            results = {
                'morphology': self.decode_predictions(predictions),
                'raw_predictions': predictions.cpu().numpy(),
                'survey': survey,
                'quality_score': self.compute_quality_score(image, predictions),
                'processing_timestamp': datetime.now().isoformat()
            }
            
            if uncertainty is not None:
                results['uncertainty'] = uncertainty.cpu().numpy()
                results['confidence'] = 1.0 - uncertainty.mean().item()
            
            if features is not None:
                results['features'] = features.cpu().numpy()
            
            return results
            
        except Exception as e:
            self.logger.error(f"Error processing {image_path}: {str(e)}")
            return {
                'error': str(e),
                'image_path': image_path,
                'timestamp': datetime.now().isoformat()
            }
    
    def batch_process(self, image_list: List[str], 
                     output_format: str = 'csv',
                     n_jobs: int = 4) -> pd.DataFrame:
        """Process multiple galaxies in parallel"""
        from joblib import Parallel, delayed
        
        # Process in parallel
        results = Parallel(n_jobs=n_jobs)(
            delayed(self.process_galaxy)(img_path) 
            for img_path in tqdm(image_list)
        )
        
        # Filter out errors
        successful = [r for r in results if 'error' not in r]
        errors = [r for r in results if 'error' in r]
        
        if errors:
            self.logger.warning(f"Failed to process {len(errors)} galaxies")
            error_df = pd.DataFrame(errors)
            error_df.to_csv('processing_errors.csv', index=False)
        
        # Create output dataframe
        df = pd.DataFrame(successful)
        
        # Expand morphology predictions
        morphology_df = pd.DataFrame(list(df['morphology']))
        df = pd.concat([df.drop('morphology', axis=1), morphology_df], axis=1)
        
        return df
    
    def create_api_endpoint(self):
        """Create REST API for model serving"""
        from flask import Flask, request, jsonify
        
        app = Flask(__name__)
        
        @app.route('/health', methods=['GET'])
        def health_check():
            return jsonify({'status': 'healthy', 'model_loaded': self.model is not None})
        
        @app.route('/predict', methods=['POST'])
        def predict():
            try:
                # Get image from request
                if 'image' not in request.files:
                    return jsonify({'error': 'No image provided'}), 400
                
                # Save temporary file
                image_file = request.files['image']
                temp_path = f"/tmp/{image_file.filename}"
                image_file.save(temp_path)
                
                # Process image
                results = self.process_galaxy(temp_path)
                
                # Clean up
                os.remove(temp_path)
                
                return jsonify(results)
                
            except Exception as e:
                return jsonify({'error': str(e)}), 500
        
        return app
```

#### Step 5.4: Monitoring and Alerts
```python
# scripts/monitoring.py
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

class ModelMonitor:
    def __init__(self, config):
        self.config = config
        self.metrics_history = []
        
    def monitor_production_performance(self, model, test_data):
        """Monitor model performance in production"""
        # Evaluate current performance
        current_metrics = self.evaluate_model(model, test_data)
        
        # Add to history
        self.metrics_history.append({
            'timestamp': datetime.now().isoformat(),
            'metrics': current_metrics
        })
        
        # Check for degradation
        if len(self.metrics_history) > 1:
            previous_metrics = self.metrics_history[-2]['metrics']
            
            degradations = []
            for metric, value in current_metrics.items():
                prev_value = previous_metrics.get(metric)
                if prev_value is not None:
                    # Check for significant degradation (>5%)
                    if abs(value - prev_value) / prev_value > 0.05:
                        degradations.append({
                            'metric': metric,
                            'previous': prev_value,
                            'current': value,
                            'change': (value - prev_value) / prev_value
                        })
            
            if degradations:
                self.alert_degradation(degradations)
        
        # Save metrics
        self.save_metrics()
        
        return current_metrics
    
    def alert_degradation(self, degradations):
        """Send alert for performance degradation"""
        # Create alert message
        message = "Performance Degradation Detected!\n\n"
        for deg in degradations:
            message += f"{deg['metric']}: {deg['previous']:.4f} â†’ {deg['current']:.4f} "
            message += f"({deg['change']*100:+.1f}%)\n"
        
        # Send email alert
        self.send_email_alert("Model Performance Alert", message)
        
        # Log to monitoring system
        self.log_to_monitoring_system(degradations)
    
    def setup_automated_monitoring(self):
        """Setup automated monitoring schedule"""
        import schedule
        
        # Daily performance check
        schedule.every().day.at("06:00").do(
            self.monitor_production_performance
        )
        
        # Weekly comprehensive evaluation
        schedule.every().monday.at("00:00").do(
            self.run_comprehensive_evaluation
        )
        
        # Monthly retraining check
        schedule.every().month.do(
            self.check_retraining_needed
        )
        
        print("Monitoring scheduled. Running...")
        
        while True:
            schedule.run_pending()
            time.sleep(3600)  # Check every hour
```

### Deliverables for Phase 5
- [ ] Active learning system reducing labeling needs by 80%
- [ ] Model registry with automated versioning
- [ ] Production API with <100ms inference time
- [ ] Comprehensive monitoring and alerting system

---

## Summary and Next Steps

### Overall Project Timeline
- **Weeks 1-2**: Foundation and baseline model
- **Weeks 3-4**: Multi-survey data collection
- **Weeks 5-6**: OOD testing framework
- **Weeks 7-8**: Domain adaptation
- **Weeks 9-10**: Production deployment

### Key Success Metrics
1. **Baseline Performance**: >90% accuracy on Galaxy Zoo
2. **OOD Robustness**: <15% degradation on DESI/HST
3. **Inference Speed**: <100ms per galaxy
4. **Active Learning**: 80% reduction in labeling needs
5. **Production Uptime**: >99.9% availability

### Future Extensions
1. **Multi-wavelength Integration**: Incorporate IR/Radio data
2. **Time-domain Analysis**: Track morphological changes
3. **Rare Object Detection**: Specialized models for peculiar galaxies
4. **Interactive Visualization**: Web-based exploration tools

### Repository Structure
```
galaxy-sommelier/
â”œâ”€â”€ configs/              # Configuration files
â”œâ”€â”€ data -> /pscratch/sd/s/sihany/galaxy-sommelier-data/
â”œâ”€â”€ models/              # Model checkpoints
â”œâ”€â”€ notebooks/           # Analysis notebooks
â”œâ”€â”€ results/             # Experimental results
â”œâ”€â”€ scripts/             # Implementation scripts
â”‚   â”œâ”€â”€ data_processing.py
â”‚   â”œâ”€â”€ model_setup.py
â”‚   â”œâ”€â”€ train_baseline.py
â”‚   â”œâ”€â”€ download_desi_data.py
â”‚   â”œâ”€â”€ ood_diagnostics.py
â”‚   â”œâ”€â”€ domain_adaptation.py
â”‚   â”œâ”€â”€ active_learning.py
â”‚   â””â”€â”€ production_pipeline.py
â”œâ”€â”€ tests/               # Unit tests
â”œâ”€â”€ docs/                # Documentation
â”œâ”€â”€ requirements.txt     # Dependencies
â””â”€â”€ README.md           # Project documentation
```

This comprehensive plan provides a clear roadmap from initial setup through production deployment, with specific implementation details for each component.