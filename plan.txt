# Galaxy Sommelier: Vision Transformer for Galaxy Morphology Classification

## Project Overview
Building a state-of-the-art galaxy morphology classifier using DINOv2 fine-tuning that generalizes well across different astronomical surveys.

**Environment**: NERSC Perlmutter (A100 GPUs)  
**Data Storage**: `/pscratch/sd/s/sihany/galaxy-sommelier-data/`  
**Target**: Universal cross-survey morphology classifier  

---

## Current Status: Phase 3 Development ðŸš§

### 26-Feature Standardization Project âœ… COMPLETED
- **Problem**: Models trained on different feature sets (SDSS: 74, Mixed: 52, Triple: 14) making comparisons invalid
- **Solution**: Identified 26 overlapping morphological features across all surveys (SDSS, DECaLS, HST, UKIDSS)
- **Result**: All configurations now use standardized 26-feature output for fair scientific comparison

### Triple Mixed Dataset Development âœ… COMPLETED  
- **Implementation**: `TripleMixedDataset` class supporting three-way survey mixing (33% SDSS, 33% DECaLS, 34% HST)
- **Files**: `scripts/triple_mixed_dataset.py`, `configs/triple_mixed_*.yaml`

### LoRA Implementation âœ… COMPLETED
- **Implementation**: Created `train_lora.py` with conservative LoRA settings (rank=16, alpha=32)
- **Benefits**: Parameter-efficient training (~1-5% trainable parameters), faster convergence, better generalization
- **Files**: `scripts/train_lora.py`, `configs/lora_head_training_config.yaml`, `scripts/run_lora_training.sh`
- **Integration**: Minimal changes to existing codebase, reuses all data loading and loss functions

### Current Issues ðŸš§
1. **Dataset Sizing**: Using arbitrary dataset sizes without scientific justification
   - **Need**: Implement data-driven sizing using `min(available_catalogs) * N` methodology
2. **Model Comparison Bug**: IndexError in `compare_models.py` when comparing models with different feature counts
   - **Error**: `IndexError: index 53 is out of bounds for axis 1 with size 52`
   - **Cause**: Feature mapping inconsistency between 52-feature and 26-feature models

---

## Key Results Summary

### Phase 1: Foundation âœ… COMPLETED
- **Training Progression**: Baseline (r=0.116) â†’ Head-trained (r=0.759) â†’ Fine-tuned (r=0.850)
- **Best Performance**: Correlation = 0.85, RÂ² = 0.72, MAE = 0.106
- **Strong Features**: Disk detection (r=0.968), Edge-on (r=0.935), Spiral (r=0.857)

### Phase 2: OOD Generalization Study âœ… COMPLETED
- **Hypothesis Confirmed**: Mixed-survey training improves cross-survey generalization
- **Key Result**: Mixed model (SDSS+DECaLS) vs SDSS-only on UKIDSS test: **0.857 vs 0.819 correlation**
- **Tools**: Unified evaluation script `scripts/compare_models.py`

---

## Repository Structure

```
galaxy-sommelier/
â”œâ”€â”€ configs/                    # Training configurations (26-feature standardized)
â”‚   â”œâ”€â”€ lora_head_training_config.yaml  # LoRA training configuration
â”‚   â””â”€â”€ ...                     # Other configs
â”œâ”€â”€ scripts/                    # Core implementation
â”‚   â”œâ”€â”€ compare_models.py       # Model evaluation and comparison (needs bug fix)
â”‚   â”œâ”€â”€ train_lora.py          # LoRA training script
â”‚   â”œâ”€â”€ run_lora_training.sh    # LoRA training pipeline
â”‚   â”œâ”€â”€ triple_mixed_dataset.py # Three-survey dataset implementation
â”‚   â”œâ”€â”€ sdss_dataset.py         # SDSS-only dataset
â”‚   â”œâ”€â”€ mixed_dataset.py        # SDSS+DECaLS dataset
â”‚   â””â”€â”€ standard_26_features.py # Feature standardization
â”œâ”€â”€ models/                     # Saved model checkpoints
â”œâ”€â”€ benchmark_results/          # Performance evaluation results
â”œâ”€â”€ plots/                      # Visualization outputs
â”œâ”€â”€ feature_registry.py         # Feature mapping registry
â”œâ”€â”€ four_survey_feature_mapping.csv  # Cross-survey feature mapping
â””â”€â”€ requirements.txt            # Updated with peft>=0.7.0
```

## Data Storage Structure ($PSCRATCH)

```
/pscratch/sd/s/sihany/galaxy-sommelier-data/
â”œâ”€â”€ catalogs/                   # Galaxy Zoo catalogs
â”‚   â”œâ”€â”€ gz2_hart16.csv         # SDSS Galaxy Zoo 2
â”‚   â”œâ”€â”€ decals_gz_catalog.csv  # DECaLS Galaxy Zoo
â”‚   â””â”€â”€ hst_gz_catalog.csv     # HST Galaxy Zoo
â”œâ”€â”€ images/                     # Galaxy images by survey
â”‚   â”œâ”€â”€ sdss/                  # SDSS .fits files
â”‚   â”œâ”€â”€ decals/                # DECaLS .fits files
â”‚   â””â”€â”€ hst/                   # HST .fits files
â”œâ”€â”€ models/                     # Trained model storage
â”‚   â”œâ”€â”€ best_model.pt          # SDSS-only (74 features)
â”‚   â”œâ”€â”€ mixed/best_model.pt    # SDSS+DECaLS (52 features)
â”‚   â”œâ”€â”€ lora/                  # LoRA model storage
â”‚   â””â”€â”€ triple/                # Future triple-mixed models
â””â”€â”€ processed/                  # Preprocessed data cache
```

---

## Immediate Next Steps

### 1. Fix Model Comparison Bug ðŸ”¥ HIGH PRIORITY
- **Task**: Fix IndexError in `compare_models.py` for different feature count models
- **Issue**: Feature mapping fails when comparing 52-feature vs 26-feature models
- **Solution**: Update feature intersection logic to handle mismatched dimensions

### 2. Test LoRA Performance
- **Task**: Train and evaluate LoRA model on SDSS dataset
- **Command**: `./scripts/run_lora_training.sh`
- **Goal**: Compare LoRA vs full fine-tuning efficiency and performance

### 3. Fix Dataset Sizing
- **Task**: Update all configs with principled dataset sizing approach
- **Method**: Use `min(available_catalogs) * N` for consistent, data-driven sizes
- **Files to Update**: All config files in `configs/`

### 4. Comprehensive Benchmarking
- **Task**: Run fixed `compare_models.py` on all standardized models
- **Test Set**: UKIDSS (held-out survey)
- **Models**: Baseline, LoRA, Mixed (once comparison bug is fixed)

---

## Future Phases

### Phase 4: Cross-Survey Evaluation
- Test all models on UKIDSS with standardized 26 features
- Quantify improvement from survey diversity
- Document final scientific conclusions

### Phase 5: Production Deployment
- Model registry and versioning system
- REST API for morphology classification
- Automated monitoring and performance tracking

---

## Key Lessons Learned

1. **Feature Standardization Critical**: Models must predict identical feature sets for valid comparison
2. **Dataset Sizing Principles**: Use data-driven methodology, not arbitrary sizes
3. **Survey Diversity Helps**: Mixed training significantly improves cross-survey generalization
4. **LoRA Benefits**: Parameter-efficient fine-tuning with minimal code changes, faster training, better generalization potential
5. **Model Comparison Robustness**: Need robust feature mapping logic to handle models with different output dimensions
6. **Fair Comparison Essential**: Scientific conclusions require controlled experimental conditions